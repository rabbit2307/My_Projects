# Machine Learning Projects
This repository contains a machine learning project focused on classifying astronomical objects (Stars, Galaxies, and Quasars) based on their photometric and spectral features. The project demonstrates a complete workflow from data preprocessing and model training to evaluation and individual sample inference. The primary goal of this project is to build and evaluate various machine learning models capable of accurately classifying celestial objects. The dataset used contains features derived from astronomical observations, such as magnitudes in different filters (u, g, r, i, z), redshift, and observation metadata. 
The project aims to provide a clear and reproducible pipeline for this classification task. Features include Data Preprocessing (handling missing values, encoding categorical labels, feature scaling, managing new/unknown data points), Model Training (demonstrating the training of multiple classification algorithms), Model Evaluation (providing comprehensive evaluation metrics including Accuracy Score, Classification Report with Precision, Recall, F1-score for each class, Confusion Matrix visualization, and Prediction Confidence visualization with histograms of maximum probabilities), and Individual Sample Inference (a utility function and examples to classify single, new astronomical objects and display their predicted class and probability distributions). 
The project explores the performance of Random Forest Classifier, Support Vector Machine (SVM), XGBoost Classifier, and Multi-layer Perceptron (Neural Network). Repository contents include Predicting Stars, Galaxies & Quasars with ML Model.ipynb (the main Jupyter Notebook), label_encoder.pkl (a serialized LabelEncoder object), scaler.pkl (a serialized StandardScaler object), rf_model.pkl (the trained Random Forest Classifier model), svm_model.pkl (the trained Support Vector Machine model), xgb_sdss_classifier.pkl (the trained XGBoost Classifier model), nn_model.pkl (the trained Neural Network model), Skyserver_12_30_2019_4_49_58_PM.csv (the dataset), and .ipynb_checkpoints/ (Jupyter Notebook's auto-saved checkpoints). To run this project locally, you would clone the repository, create and activate a virtual environment, install dependencies (pandas, numpy, scikit-learn, matplotlib, seaborn, xgboost, and optionally tensorflow/keras), launch Jupyter Notebook, open the main notebook, and run its cells sequentially. For any questions or suggestions, feel free to reach out.
